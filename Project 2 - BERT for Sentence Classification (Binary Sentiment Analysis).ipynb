{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef94f715",
   "metadata": {},
   "source": [
    "# Setup\n",
    "### Import necessary packages and libraries, set model to train on GPU, and load BERT pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c754fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages and libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ae889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Run model under GPU for faster training and testing\n",
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device = tf.test.gpu_device_name()\n",
    "\n",
    "# Confirm\n",
    "if device == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d013a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Found: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# Assign PyTorch to run off GPU\n",
    "gpu = torch.device(\"cuda\")\n",
    "print('GPU Found:', torch.cuda.get_device_name(0))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "031417c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "# Display Pytorch and CUDA versions\n",
    "print('PyTorch version: {}'.format(torch.__version__))\n",
    "print('CUDA version: {}'.format(torch.version.cuda))\n",
    "\n",
    "# limit how much GPU memory PyTorch can use\n",
    "torch.cuda.set_per_process_memory_fraction(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c379135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load BERT tokenizer and model using GPU\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',     # 12 layer BERT model with uncased vocabulary\n",
    "                                                      num_labels=2)            # 2 for binary classification\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19950849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "---- Embedding Layer ----\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "---- First Transformer ----\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "---- Output Layer ----\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('---- Embedding Layer ----\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n---- First Transformer ----\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n---- Output Layer ----\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54925402",
   "metadata": {},
   "source": [
    "# Preparation - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0dcf9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Stuning even for the non-gamer</td>\n",
       "      <td>This sound track was beautiful! It paints the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!</td>\n",
       "      <td>This soundtrack is my favorite music of all ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellent Soundtrack</td>\n",
       "      <td>I truly like this soundtrack and I enjoy video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>If you've played the game, you know how divine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      2                     Stuning even for the non-gamer   \n",
       "1      2              The best soundtrack ever to anything.   \n",
       "2      2                                           Amazing!   \n",
       "3      2                               Excellent Soundtrack   \n",
       "4      2  Remember, Pull Your Jaw Off The Floor After He...   \n",
       "\n",
       "                                              review  \n",
       "0  This sound track was beautiful! It paints the ...  \n",
       "1  I'm reading a lot of reviews saying that this ...  \n",
       "2  This soundtrack is my favorite music of all ti...  \n",
       "3  I truly like this soundtrack and I enjoy video...  \n",
       "4  If you've played the game, you know how divine...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load datasets\n",
    "df_total = pd.read_csv('amazon_review_polarity_csv/train.csv', header=None, names=['label', 'title', 'review'])\n",
    "\n",
    "'''\n",
    "The Amazon reviews polarity dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu). \n",
    "It is used as a text classification benchmark in the following paper: \n",
    "Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
    "\n",
    "I am using the subset of data provided by Kaggle user 'kritanjalijain', but it too large for my computer to train in a \n",
    "reasonable amount of time. I will only be using a random 10% of the data.\n",
    "\n",
    "'''\n",
    "\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a0ce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of truncated dataset: 853,214\n"
     ]
    }
   ],
   "source": [
    "# Preprocess dataframes to be of reviews with max_length = 200\n",
    "\n",
    "max_length = 200\n",
    "\n",
    "df_total = df_total[df_total['review'].apply(lambda x: len(x) <= max_length)]\n",
    "\n",
    "print('Length of truncated dataset: {:,}'.format(len(df_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "025bdffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset for training and validation: 42,661\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 10% of the dataset\n",
    "df_train = df_total.sample(frac=0.05, random_state=42)\n",
    "print('Length of dataset for training and validation: {:,}'.format(len(df_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79511472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Get the lists of reviews and their labels.\n",
    "reviews = df_train.review.values\n",
    "\n",
    "# Subtracting 1 to convert to a binary dataset with values 1 and 0\n",
    "# Original dataset had 1's and 2's which is not recognized by BERT's binary classification model\n",
    "labels = df_train.label.values-1   \n",
    "print(type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00357756",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Original:  If you want an eye opening, heart opener this is the book for you. One I will read again, and again, and again.\n",
      "  Tokenized:  ['if', 'you', 'want', 'an', 'eye', 'opening', ',', 'heart', 'opener', 'this', 'is', 'the', 'book', 'for', 'you', '.', 'one', 'i', 'will', 'read', 'again', ',', 'and', 'again', ',', 'and', 'again', '.']\n",
      "  Token IDs:  [2065, 2017, 2215, 2019, 3239, 3098, 1010, 2540, 16181, 2023, 2003, 1996, 2338, 2005, 2017, 1012, 2028, 1045, 2097, 3191, 2153, 1010, 1998, 2153, 1010, 1998, 2153, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Visualize tokenization of review\n",
    "# Print the original sentence.\n",
    "print('  Original: ', reviews[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('  Tokenized: ', tokenizer.tokenize(reviews[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('  Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(reviews[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "915ef8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max review length in dataset:  108\n"
     ]
    }
   ],
   "source": [
    "# Determine max review length\n",
    "max_len = 0\n",
    "\n",
    "for text in reviews:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max review length in dataset: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62b1bc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  If you want an eye opening, heart opener this is the book for you. One I will read again, and again, and again.\n",
      "Token IDs: tensor([  101,  2065,  2017,  2215,  2019,  3239,  3098,  1010,  2540, 16181,\n",
      "         2023,  2003,  1996,  2338,  2005,  2017,  1012,  2028,  1045,  2097,\n",
      "         3191,  2153,  1010,  1998,  2153,  1010,  1998,  2153,  1012,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize training dataset\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in reviews:\n",
    "    if len(text) > max_length:\n",
    "        text = text[:max_length]                                                       # truncate longer sequences\n",
    "    encoded_dict = tokenizer.encode_plus(text,                                         # text to encode\n",
    "                                         add_special_tokens=True,                      # add '[CLS]' and '[SEP]'\n",
    "                                         truncation=True,\n",
    "                                         max_length=max_length,                        # truncate all sentences\n",
    "                                         padding='max_length',                         # pad to max_length\n",
    "                                         return_attention_mask=True,                   # Construct attn. masks\n",
    "                                         return_tensors='pt')                          # Return pytorch tensors\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print('Original Text: ', reviews[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab077436",
   "metadata": {},
   "source": [
    "# Preparation - Pre-Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1912027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  38,394 training samples\n",
      "   4,267 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Combine the training inpurts into a TensorDataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Calculate size of each dataset\n",
    "training_size = int(0.9 * len(dataset))\n",
    "validation_size = len(dataset) - training_size\n",
    "\n",
    "# Split total dataset into training and validation sets\n",
    "training_set, validation_set = random_split(dataset, [training_size, validation_size])\n",
    "\n",
    "# Create a dataloader to batch and shuffle the data (split by training and testing data)\n",
    "# Randomly sample datasets using RandomSampler\n",
    "\n",
    "\n",
    "print('{:>8,} training samples'.format(training_size))\n",
    "print('{:>8,} validation samples'.format(validation_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c380d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine batch size for training\n",
    "batch_size = 20\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_dataloader = DataLoader(training_set, sampler=RandomSampler(training_set), batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(validation_set, sampler=SequentialSampler(validation_set), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c096e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr = 5e-5,        # learning rate default value\n",
    "                              eps = 1e-8)       # epsilon default value\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()                    # measures difference between predicted vs. actual probability dist for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "483be8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader)*epochs\n",
    "\n",
    "# Create learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bee3021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function for calculating accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "# Citation: BERT Fine-Tuning Tutorial with PyTorch (Chris McCormick, Nick Ryan; 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "670bbaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function for formatting elapsed time as hh:mm:ss\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes time in seconds and returns string in format hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "# Citation: BERT Fine-Tuning Tutorial with PyTorch (Chris McCormick, Nick Ryan; 2020)\n",
    "# I found these functions to be quite intuitive for visualizing the training process so I wanted to add them into this notebook and credit them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f75be",
   "metadata": {},
   "source": [
    "# Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b96ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Epoch 1 / 3 -----\n",
      "Training...\n",
      "  Batch  500 of 1,920. Elapsed: 0:03:17\n",
      "  Batch 1,000 of 1,920. Elapsed: 0:06:43\n",
      "  Batch 1,500 of 1,920. Elapsed: 0:10:06\n",
      "\n",
      "    Average training loss: 0.21\n",
      "    Training epoch took: 0:12:55\n",
      "\n",
      "Running Validation...\n",
      "    Accuracy: 0.94\n",
      "    Validation Loss: 0.18\n",
      "    Validation Time: 0:00:27\n",
      "\n",
      "----- Epoch 2 / 3 -----\n",
      "Training...\n",
      "  Batch  500 of 1,920. Elapsed: 0:03:18\n",
      "  Batch 1,000 of 1,920. Elapsed: 0:06:52\n",
      "  Batch 1,500 of 1,920. Elapsed: 0:10:29\n",
      "\n",
      "    Average training loss: 0.10\n",
      "    Training epoch took: 0:13:30\n",
      "\n",
      "Running Validation...\n",
      "    Accuracy: 0.94\n",
      "    Validation Loss: 0.20\n",
      "    Validation Time: 0:00:30\n",
      "\n",
      "----- Epoch 3 / 3 -----\n",
      "Training...\n",
      "  Batch  500 of 1,920. Elapsed: 0:03:31\n",
      "  Batch 1,000 of 1,920. Elapsed: 0:06:51\n",
      "  Batch 1,500 of 1,920. Elapsed: 0:10:10\n",
      "\n",
      "    Average training loss: 0.04\n",
      "    Training epoch took: 0:12:58\n",
      "\n",
      "Running Validation...\n",
      "    Accuracy: 0.94\n",
      "    Validation Loss: 0.27\n",
      "    Validation Time: 0:00:28\n",
      "\n",
      "Training complete\n",
      "Total training time: 0:40:48 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# Switch model to training mode\n",
    "model.train()\n",
    "\n",
    "# Set up training metrics\n",
    "training_stats = []     # training, validation loss, accuracy, timings\n",
    "total_t0 = time.time()  # total training time for entire run\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #-----------------\n",
    "    #    TRAINING\n",
    "    #-----------------\n",
    "    \n",
    "    # reset total loss per epoch\n",
    "    total_train_loss = 0.0     \n",
    "\n",
    "    print('')\n",
    "    print('----- Epoch {:} / {:} -----'.format(epoch+1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes\n",
    "    t0 = time.time()     \n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader): \n",
    "        \n",
    "        # Progress update every 400 batches because my computer is slow and I want to know if it's working or not...\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time()-t0)\n",
    "            print('  Batch {:>4,} of {:>4,}. Elapsed: {:}'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "        # Separate into 3 pytorch tensors\n",
    "        batch_input_ids = batch[0].to(gpu)\n",
    "        batch_attention_masks = batch[1].to(gpu)\n",
    "        batch_labels = batch[2].to(gpu)\n",
    "        \n",
    "        # Forward pass (evaluate the model on training batch)\n",
    "        output = model(batch_input_ids,\n",
    "                       attention_mask=batch_attention_masks, \n",
    "                       labels=batch_labels)\n",
    "        loss = output.loss\n",
    "        \n",
    "        # Total training loss over all batches; `.item()` returns the python value from the tensor\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Backward pass (calculate gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0 to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Clear previously calculated gradients\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        \n",
    "    # Calculate average loss over all batches\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \n",
    "    # Measure how long the epoch took to train\n",
    "    train_time = format_time(time.time()-t0)    # format_time(time.time()-t0)\n",
    "    \n",
    "    print('')\n",
    "    print('    Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('    Training epoch took: {:}'.format(train_time))\n",
    "    \n",
    "    \n",
    "    #--------------------\n",
    "    #     VALIDATION\n",
    "    #--------------------\n",
    "    \n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Tracking metrics\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    # Evaluate data for 1 epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Separate into 3 pytorch tensors\n",
    "        batch_input_ids = batch[0].to(gpu)\n",
    "        batch_attention_masks = batch[1].to(gpu)\n",
    "        batch_labels = batch[2].to(gpu)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(batch_input_ids,\n",
    "                           attention_mask=batch_attention_masks,\n",
    "                           labels=batch_labels)\n",
    "        \n",
    "        # Get loss and logits output by model\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        \n",
    "        #Move logits and labels to CPI\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = batch_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Total training loss over all abtches\n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy for this batch; total for all batches\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    # Report final accuracy for evaluation run\n",
    "    avg_eval_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print('    Accuracy: {0:.2f}'.format(avg_eval_accuracy))\n",
    "    \n",
    "    # Calculate average loss over all batches\n",
    "    avg_eval_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Calculate time for validation\n",
    "    eval_time = format_time(time.time()-t0)\n",
    "    print('    Validation Loss: {0:.2f}'.format(avg_eval_loss))\n",
    "    print('    Validation Time: {:}'.format(eval_time))\n",
    "    \n",
    "    # Record all statistics from this epoch\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch+1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Evaluation Loss': avg_eval_loss,\n",
    "            'Evaluation Accuracy': avg_eval_accuracy,\n",
    "            'Training Time': train_time,\n",
    "            'Validation Time': eval_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print('')\n",
    "print('Training complete')\n",
    "print('Total training time: {:} (h:mm:ss)'.format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6470af4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Evaluation Loss</th>\n",
       "      <th>Evaluation Accuracy</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0:12:55</td>\n",
       "      <td>0:00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0:13:30</td>\n",
       "      <td>0:00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0:12:58</td>\n",
       "      <td>0:00:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Evaluation Loss  Evaluation Accuracy Training Time  \\\n",
       "epoch                                                                      \n",
       "1               0.21             0.18                 0.94       0:12:55   \n",
       "2               0.10             0.20                 0.94       0:13:30   \n",
       "3               0.04             0.27                 0.94       0:12:58   \n",
       "\n",
       "      Validation Time  \n",
       "epoch                  \n",
       "1             0:00:27  \n",
       "2             0:00:30  \n",
       "3             0:00:28  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display training summary\n",
    "\n",
    "# Create a DataFrame from training stats\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Initially, the intention was to do multiple epochs, but since it took so long to do one and the accuracy came out \n",
    "# high, I decided to only do 1 for this execise. With how many data points were in the training and testing files,\n",
    "# doing 1 epoch should suffice. With low loss during training and evaluation, i feel comfortable with how this data \n",
    "# was trained\n",
    "df_stats = df_stats.set_index('epoch').round(2)\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d060d212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: C:\\Users\\leeha\\PycharmProjects\\twitter_bias\\model.pkl\n"
     ]
    }
   ],
   "source": [
    "# SAVE THE MODEL!!!!!!\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "print('Model saved: ' + os.getcwd() + '\\model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdf5101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: C:\\Users\\leeha\\PycharmProjects\\twitter_bias\\model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load the model. This is now a major checkpoint after the model has been trained and validated\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "print('Model loaded: ' + os.getcwd() + '\\model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387df7bc",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "320e41e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test reviews: 40,000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df_test = pd.read_csv('amazon_review_polarity_csv/test.csv', header=None, names=['label', 'title', 'review'])\n",
    "\n",
    "# Truncate testing dataset\n",
    "df_test = df_test.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test reviews: {:,}\\n'.format(df_test.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "reviews = df_test.review.values\n",
    "labels = df_test.label.values - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5b58255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length in subset:  332\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "for text in reviews:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length in subset: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc690ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every sentence...\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# Set max length based on previous cell result\n",
    "\n",
    "max_length = 260 \n",
    "\n",
    "for text in reviews:\n",
    "    if len(text) > max_length:\n",
    "        text = text[:max_length]    # truncate longer sequences\n",
    "    encoded_dict = tokenizer.encode_plus(text,                                         # text to encode\n",
    "                                         add_special_tokens=True,                      # add '[CLS]' and '[SEP]'\n",
    "                                         truncation=True,\n",
    "                                         max_length=max_length,                        # truncate all sentences\n",
    "                                         padding='max_length',                          # pad to max_length\n",
    "                                         return_attention_mask=True,                   # Construct attn. masks\n",
    "                                         return_tensors='pt')                          # Return pytorch tensors\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ded5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 20  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=SequentialSampler(prediction_data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67bcb28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 40,000 test sentences...\n",
      "---- DONE\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model back into evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Set tracking variables for predictions \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Run predictions \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(gpu) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    batch_input_ids, batch_input_mask, batch_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(batch_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=batch_input_mask,\n",
    "                     return_dict=True)\n",
    "\n",
    "    logits = result.logits\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = batch_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "    \n",
    "# Print when done\n",
    "print('---- DONE')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18978d72",
   "metadata": {},
   "source": [
    "# Calculating Accuracy, Precision, and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed1d9c",
   "metadata": {},
   "source": [
    "Accuracy: Comparing predicted labels to true labels\n",
    "<br>\n",
    "Precision: Measures proportion of true positives among all positive predictions\n",
    "<br>\n",
    "Recall: Measures the proportion of true positives that were correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5e91eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction datatype:  <class 'list'>\n",
      "Prediction sample:  [array([[-2.1500902,  3.0271022],\n",
      "       [ 3.1468945, -3.380125 ],\n",
      "       [-1.788168 ,  2.6663263],\n",
      "       [-2.3615484,  3.1593673],\n",
      "       [ 3.1779842, -3.4317043],\n",
      "       [ 3.189553 , -3.4439228],\n",
      "       [ 3.0414546, -3.2914834],\n",
      "       [-2.1212173,  2.9977617],\n",
      "       [-2.3401198,  3.1620266],\n",
      "       [ 3.0207577, -3.2524347],\n",
      "       [-2.401752 ,  3.191451 ],\n",
      "       [ 3.1120658, -3.3415437],\n",
      "       [ 3.2073267, -3.481062 ],\n",
      "       [ 3.153405 , -3.4042554],\n",
      "       [ 3.074229 , -3.2698395],\n",
      "       [ 3.1316717, -3.37944  ],\n",
      "       [-2.582051 ,  3.239312 ],\n",
      "       [ 3.1050615, -3.3385754],\n",
      "       [-2.2124133,  3.0744362],\n",
      "       [ 3.1532168, -3.4060702]], dtype=float32)]\n",
      "\n",
      "\n",
      "\n",
      "True label datatype:  <class 'list'>\n",
      "True label sample:  [array([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
      "      dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "# Lets see what we're working with...\n",
    "print('Prediction datatype: ', type(predictions))\n",
    "print('Prediction sample: ', predictions[:1])\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "print('True label datatype: ', type(true_labels))\n",
    "print('True label sample: ', true_labels[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e22fac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:        0.891\n",
      "Precision score:       0.9027\n",
      "Recall score:          0.8784\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision, accuaracy, and recall scores\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "accuracy_set = []\n",
    "precision_set = []\n",
    "recall_set = []\n",
    "\n",
    "for i in range(len(true_labels)):\n",
    "    pred_labels = np.argmax(predictions[i], axis=1).flatten()\n",
    "    \n",
    "    accuracy_set.append(accuracy_score(true_labels[i], pred_labels))\n",
    "    precision_set.append(precision_score(true_labels[i], pred_labels))\n",
    "    recall_set.append(recall_score(true_labels[i], pred_labels, pos_label=1))\n",
    "\n",
    "print('Accuracy score: {:>12.4}'.format(sum(accuracy_set) / len(accuracy_set)))\n",
    "print('Precision score: {:>12.4}'.format(sum(precision_set) / len(precision_set)))\n",
    "print('Recall score: {:>15.4}'.format(sum(recall_set) / len(recall_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cada00f",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42baa02",
   "metadata": {},
   "source": [
    "Pretty good scores! I'm quite happy with the results since I used a random sample of a large dataset, which could be prone to errors and using too much of one class. With the full dataset, I would hope for higher scores, but this is definitely a satisfactory score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
